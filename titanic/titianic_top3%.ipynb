{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 背景介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经历前一段时间对titanic数据集的学习，有以下总结:\n",
    "\n",
    "- Cabin 这个特征很难对于结果的帮助是有限的，比如尝试了'IF_Cabin'特征，如果乘客没有Cabin就为0，有就为1\n",
    "- 构建 Deck 特征没有帮助.根据在客舱栏中找到的字母，我们可以设计一个甲板特征，指示乘客在哪个甲板上（A-G，T或U表示未知）。但是噪音很大，对分数没有帮助\n",
    "- Embarked 没有帮助，我不知道为什么很多人甚至把它包括在他们的里。它对生存机会没有影响\n",
    "- 分类特征转换问题上，如果将分类特征转换为顺序特征（例如将pclass转换为pclass_1、pclass_2和pclass_3具有可能值0、1的特征），某些算法的性能可能会更好。优点是在某些情况下精度更高，缺点是-你失去了pclass之间的关系（意味着算法会认为那些是独立的、无序的类，而实际上它们是有序的-pclass=1比pclass=3“更好”），你添加的维并不总是好的，存在维的诅咒。在我的具体案例中，将pclass转换为3个功能并没有帮助\n",
    "- 在Fare和Age上做分区是有一定的帮助的\n",
    "- 归一化和标准化。标准化是助于提高分数。缩放功能对许多ML算法（如KNN）很有帮助，例如，它确实提高了它们的分数。这种转化器假定特性的正态分布，但有时minmaxscaler可能更好，标准化受到离群值影响效果较小\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "168a63fe-1d3d-41bd-94fa-e0105eb822d5",
    "_uuid": "50ea2d24dbba86dff0ccf041890fe36df6b4e363"
   },
   "source": [
    "# 关于 Kaggle 的分数\n",
    "我花了很多时间坚持不懈地努力提高分数，即使是很小的一点，从0.76到0.83（目前为止）,我的目标应该是用可靠的评分构建最健壮和可归纳的模型，但不一定是最佳评分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "003d7296-699b-46e6-b645-64aeaf021c3b",
    "_uuid": "c98fe142813df3d1e51c5b2a551bde15dcfeebe1"
   },
   "source": [
    "## 模块读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e12020f7-4f94-4ecc-9007-9b7a6e7458a6",
    "_uuid": "1fecb0980d8d422ec0f005c4bfd6225385c2c60f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Dataframe operations\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d49e43e8-0dc0-41b7-afd0-60acc96e9f07",
    "_uuid": "4ecd55c5bd48390d026eeb6ae8de0a7ace0d4ada"
   },
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing the data on data_df\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "data_df = train_df.append(test_df) # train + test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1309, 12) (891, 12) (418, 11)\n"
     ]
    }
   ],
   "source": [
    "print(data_df.shape,train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "687a06ef-2686-4772-ac24-5e413adbda6d",
    "_uuid": "3846eff13d723fa6ff10117fc3ebc46f266b210f"
   },
   "source": [
    "# 特征工程\n",
    "\n",
    " - **Age**\n",
    " \n",
    "基于Name提取了称号的关键词，按groupby('Title')的中位数来填充Age，以便更准确地输入年龄。使用中位数是因为年龄分布并不总是正态的，所以它通常优于平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "46535259-a34c-4305-b19e-b6772834c2b1",
    "_uuid": "811b1c66a647266d203e54175918a2c906e98ee5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['Title'] = data_df['Name']\n",
    "# Cleaning name and extracting Title\n",
    "for name_string in data_df['Name']:\n",
    "    data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "\n",
    "# Replacing rare titles with more common ones\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "data_df.replace({'Title': mapping}, inplace=True)\n",
    "titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n",
    "for title in titles:\n",
    "    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\n",
    "    \n",
    "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
    "train_df['Age'] = data_df['Age'][:891]\n",
    "test_df['Age'] = data_df['Age'][891:]\n",
    "\n",
    "# Dropping Title feature\n",
    "data_df.drop('Title', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c6142e2-e351-4063-81a6-4836b4ab1b18",
    "_uuid": "ee4ddbb2ef693943bb766083c4bfc1ef4f173103"
   },
   "source": [
    " - **Adding Family_Size**\n",
    " \n",
    "Family_Size =  Parch + SibSp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "c181cefb-edd6-4d46-95c8-67b89d83ca1c",
    "_uuid": "d562760d23fa303cb93069f339bbab01fcd96486",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\n",
    "\n",
    "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
    "train_df['Family_Size'] = data_df['Family_Size'][:891]\n",
    "test_df['Family_Size'] = data_df['Family_Size'][891:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "463912c1-1f64-4fb4-9196-969f29dbbd8a",
    "_uuid": "984ac1ddd587b2aec9d520f3281c3d52208de7e8"
   },
   "source": [
    " - **Adding Family_Survival**\n",
    " \n",
    " 这个特征学习自 [S.Xu's kernel](https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever), 他将家庭为单位来计算家庭生存率，如何判断为一个家庭，用last_name和Fare来groupby，得到各个家庭是否全部活着--1，还是全部死了--0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "5939ba63-cead-4af6-86b9-587f433d63ec",
    "_uuid": "d01422f5894c381d88e036808a5aa1ba9f48b00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with family survival information: 420\n"
     ]
    }
   ],
   "source": [
    "data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "data_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\n",
    "\n",
    "DEFAULT_SURVIVAL_VALUE = 0.5\n",
    "data_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "\n",
    "for grp, grp_df in data_df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']): # the same，first return the type of values, second return others cols\n",
    "    \n",
    "    if (len(grp_df) != 1):\n",
    "        # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():  # iterrows() return index,series\n",
    "            smax = grp_df.drop(ind)['Survived'].max()\n",
    "            smin = grp_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin==0.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "\n",
    "print(\"Number of passengers with family survival information:\", \n",
    "      data_df.loc[data_df['Family_Survival']!=0.5].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "8f31cc67-1425-4e2b-8a29-4fa97b87dcdc",
    "_uuid": "3e19ae6012724aa5f8cee2b1b69af975ad42ac4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passenger with family/group survival information: 546\n"
     ]
    }
   ],
   "source": [
    "for _, grp_df in data_df.groupby('Ticket'): #only groupby('Ticket')  _, grp_df, _ return the ticket.values after group; grp_df return others cols, bin by 1,2,3\n",
    "#     print(_,grp_df)\n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "#             print(ind,row)\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin==0.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                        \n",
    "print(\"Number of passenger with family/group survival information: \" \n",
    "      +str(data_df[data_df['Family_Survival']!=0.5].shape[0]))\n",
    "\n",
    "# # Family_Survival in TRAIN_DF and TEST_DF:\n",
    "train_df['Family_Survival'] = data_df['Family_Survival'][:891]\n",
    "test_df['Family_Survival'] = data_df['Family_Survival'][891:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f5064af-21a0-4172-9565-24345d232db8",
    "_uuid": "b903a6f5c2bcfb587a722c4903c84389cae1b163"
   },
   "source": [
    " - **Making FARE BINS**\n",
    " \n",
    "FareBin = 3 is indeed greater than FareBin = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "_cell_guid": "084602ad-6739-441a-a153-23ed0de0a2df",
    "_uuid": "87e965a43b010f8589d3559c13829c5f8a7445d0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\n",
    "\n",
    "# Making Bins\n",
    "data_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n",
    "\n",
    "train_df['FareBin_Code'] = data_df['FareBin_Code'][:891]\n",
    "test_df['FareBin_Code'] = data_df['FareBin_Code'][891:]\n",
    "\n",
    "train_df.drop(['Fare'], 1, inplace=True)\n",
    "test_df.drop(['Fare'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ed9de97-5f49-48c4-91a5-a2fdd019ee7a",
    "_uuid": "94052ac922229ec3b1bd7d29ead82c2bea76e2b5"
   },
   "source": [
    " - **Making AGE BINS**\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "_cell_guid": "2c8f80c6-d271-4716-87b6-9dc31ee5f967",
    "_uuid": "8fb78bde0f082a32e9abfb21c52c67bb4d05ae3e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\n",
    "\n",
    "train_df['AgeBin_Code'] = data_df['AgeBin_Code'][:891]\n",
    "test_df['AgeBin_Code'] = data_df['AgeBin_Code'][891:]\n",
    "\n",
    "train_df.drop(['Age'], 1, inplace=True)\n",
    "test_df.drop(['Age'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef2fd090-e439-4bc0-8e96-1615ccda6f22",
    "_uuid": "8d3893970c76f066263d155207236d666e8259a8"
   },
   "source": [
    " - **Mapping SEX and cleaning data (ATTENTION!! dropping garbage) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "_cell_guid": "b5c6cdd4-3211-4032-b9e3-7a4e4dd49858",
    "_uuid": "33361df2ab42f24ba31471d2dd8b8bd49f1f1151",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "test_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "\n",
    "train_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
    "               'Embarked'], axis = 1, inplace = True)\n",
    "test_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
    "             'Embarked'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db928505-e1cf-48d0-bf35-b72a7782a774",
    "_uuid": "26631d4c02767d6f54376f07cf35b20dc1f94a58"
   },
   "source": [
    "So now our datasets look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  Family_Size  Family_Survival  FareBin_Code  \\\n",
       "0         0       3    0            1              0.5             0   \n",
       "1         1       1    1            1              0.5             4   \n",
       "2         1       3    1            0              0.5             1   \n",
       "3         1       1    1            1              0.0             4   \n",
       "4         0       3    0            0              0.5             1   \n",
       "\n",
       "   AgeBin_Code  \n",
       "0            0  \n",
       "1            3  \n",
       "2            1  \n",
       "3            2  \n",
       "4            2  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b91437e-845e-4e3f-a20e-19db3d7a8540",
    "_uuid": "52d1886521f05ac5043ef734d94aadf59cc7a073"
   },
   "source": [
    "# 训练\n",
    "\n",
    " - **Creating X and y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "_cell_guid": "2fdafbf3-62cc-4365-8168-7e149f8b51f0",
    "_uuid": "94e2b9da92605974f2e7575f85b10abf98a0d6b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_df.drop('Survived', 1)\n",
    "y = train_df['Survived']\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84ece816-177c-473e-92ae-620c5fe50be6",
    "_uuid": "fd73849d6b1805fa835b1031a1e2e812e0ef68e3"
   },
   "source": [
    " - **Scaling features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "_cell_guid": "6f077d59-c48d-41b2-82b2-7cd325ed4aab",
    "_uuid": "fc95cee7c61fee94005453e2b97598f69267ca29",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X = std_scaler.fit_transform(X)\n",
    "X_test = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd37c2af-8981-43b0-8a60-da278d130073",
    "_uuid": "e59cb01ea58d9aaf401655bee18c682976956f00"
   },
   "source": [
    " - **Grid Search CV**\n",
    " \n",
    "    use KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "_cell_guid": "0baa26fd-c625-44a0-9411-b2479ace87ab",
    "_uuid": "670108e53958cb3378a2d7f35043c698cedb05be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879492358564\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=18, p=2,\n",
      "           weights='uniform')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2400 out of 2400 | elapsed:   30.9s finished\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\n",
    "algorithm = ['auto']\n",
    "weights = ['uniform', 'distance']\n",
    "leaf_size = list(range(1,50,5))\n",
    "hyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n",
    "               'n_neighbors': n_neighbors}\n",
    "gd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n",
    "                cv=10, scoring = \"roc_auc\")\n",
    "gd.fit(X, y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f53279aa-05a3-4f61-b3c6-d54ccc9e3cf9",
    "_uuid": "b8132e9ed17b247ea14e9e0f8235d4806d411cf8"
   },
   "source": [
    "\n",
    "\n",
    "This gave my result, maybe yours diff:\n",
    "\n",
    "> KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=18, p=2, weights='uniform')\n",
    "\n",
    "This gave 0.879492358564 ROC_AUC score (not accuracy score!). 我有很多模型的ROC_AUC约为0.93-0.94，但在测试时，它们的结果大多较低!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b91368fc-7671-497c-bea2-bba3b857633c",
    "_uuid": "f2170452b8a8f2fa89f4b5e9bb221e488bf42282"
   },
   "source": [
    " - **Using a model found by grid searching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "_cell_guid": "2ab86ce3-6bc2-46d1-9324-cf496bb1ea06",
    "_uuid": "485df2df733917a8bbf405a9a9eabf792ecb89e4",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=18, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.best_estimator_.fit(X, y)\n",
    "# y_pred = gd.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6885421-d2e5-4f06-8d11-f5f4b686b251",
    "_uuid": "a64495aed8957867c0e460880fab6339c9ce981f"
   },
   "source": [
    "When I submitted the result, the model I've specified above yielded [0.82775] public score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "118adb4d-6964-43c3-b971-ecd662b159cb",
    "_uuid": "9cd8d5e04ddc8b1bbbdd0bcb947f53574fee2fc7"
   },
   "source": [
    "- **Using another K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "_cell_guid": "8adcbc6a-39e7-4fcc-a65d-c9bdd7076c49",
    "_uuid": "9bc6cdd37489b2236275c455b065f3abaed97a17",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n",
    "                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n",
    "                           weights='uniform')\n",
    "# knn.fit(X, y)\n",
    "# y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this one, I got 0.83253!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **vote**\n",
    "\n",
    "Make a little progress.Using esemble of the KNN and xgboost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Training w/bin score mean: 85.24\n",
      "Hard Voting Test w/bin score mean: 83.96\n",
      "Hard Voting Test w/bin score 3*std: +/- 5.57\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "vote_est = [\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', gd.best_estimator_),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    ('xgb', XGBClassifier()),\n",
    "    \n",
    "\n",
    "]\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, X,y, cv  = cv_split)\n",
    "vote_hard.fit(X, y)\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "y_pred = vote_hard.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a37e176c-3b55-43ab-b358-324dc384ceef",
    "_uuid": "d4d6df3e6c40063309ea72f4d4cea51cf616fd80"
   },
   "source": [
    "- **Making submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "_cell_guid": "8111500e-330c-411e-a742-66b9d4c5cb2c",
    "_uuid": "40858051e4f458835f937275be4dfe3dfa68b25f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(pd.read_csv(\"../data/test.csv\")['PassengerId'])\n",
    "temp['Survived'] = y_pred\n",
    "temp.to_csv(\"../submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "081225b5-2e95-455d-9c2c-2a4b087c156c",
    "_uuid": "dcccce817c650e7830de41704a95484c2a7b0761"
   },
   "source": [
    " ## 结果\n",
    " \n",
    " So when I submitted the score I got 0.83732, top 3%. Maybe it's enough for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3661299-8e2c-4f48-9ce2-bb9ee69457ca",
    "_uuid": "6463dc48e076a5ada4631be9aa940a7bafb3dddb"
   },
   "source": [
    "# 结论\n",
    "这里也有几点。\n",
    "- 当然，KNN不是唯一。我使用了SVMS、Adaboosing、GradientBoosting、RandomForests等，许多不同的模型，这里省略了所有这些。但knn显示了我所有设计的特征的可靠结果，所以我更喜欢它的简单。大家可以使用其他估算器来尝试数据集，特别是可以使用调优的超参数来尝试xgboost。您有可能在本内核中描述的相同功能上获得更高的分数。\n",
    "- 如果你进一步设计家庭小组，我认为你会得到更好的结果。查看[在泰坦尼克号上寻找“真实”家庭](https://www.kaggle.com/erikbruin/finding-the-real-families-on-the-titanic)了解家庭。\n",
    "- 一般来说，分组乘客是提高评分的好方法。尝试搜索组。\n",
    "- 你可以使用一些有趣的功能，比如[Slogl in Oscar's Kernel](https://www.kaggle.com/pliptor/divide-and-converse-0-82296)，试试看。\n",
    "\n",
    "\n",
    "但是，要准备好投入大量时间只为一小部分收入。似乎真正更好的模型将产生86-88+，但很难达到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
